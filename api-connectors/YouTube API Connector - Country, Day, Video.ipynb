{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1420788",
   "metadata": {},
   "source": [
    "# YouTube API Connector - Country/Day/Video Granularity\n",
    "\n",
    "First of all, let's import the libraries to connect to the YouTube API and manipulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ab8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import googleapiclient.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f34de",
   "metadata": {},
   "source": [
    "Then, let's add the credentials to connect to the YouTube Data v3 API. This will provide us a generic information about the channel, including the ID of the playlist where all the videos are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581aaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'youtube#channelListResponse', 'etag': 'xm0qCYcUUWM0F3XMlRFDWwej_n8', 'pageInfo': {'totalResults': 1, 'resultsPerPage': 5}, 'items': [{'kind': 'youtube#channel', 'etag': 'eyLQNm6fwyqFzcXjoobAzVEH-Cw', 'id': 'UCzQTrA_c1BgRNLewVyt2UFw', 'contentDetails': {'relatedPlaylists': {'likes': '', 'uploads': 'UUzQTrA_c1BgRNLewVyt2UFw'}}}]}\n"
     ]
    }
   ],
   "source": [
    "# Defining the API Key\n",
    "api_key = 'api_key'\n",
    "\n",
    "# Defining the API name and version before connection\n",
    "youtube_api_service_name = \"youtube\"\n",
    "youtube_api_version = \"v3\"\n",
    "\n",
    "youtube = build(youtube_api_service_name, youtube_api_version, developerKey = api_key)\n",
    "\n",
    "# Gathering the channel content details\n",
    "request = youtube.channels().list(part='ContentDetails', id = 'UCzQTrA_c1BgRNLewVyt2UFw')\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aba4ad",
   "metadata": {},
   "source": [
    "In the next step, this playlist ID will allow to provide us to get all the video IDs, the title, the description, and the publish date. I have created a function to gather all this data for this step.\n",
    "\n",
    "As it is only possible to get the data from only 50 videos, it is necessary to use the 'nextPageToken' to extract the data from the rest of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75bddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding read permissions to the API\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Creating a function to gather all the video information from the previous playlistId\n",
    "def gather_youtube_videos(playlistId):\n",
    "\n",
    "    youtube = build(youtube_api_service_name, youtube_api_version, developerKey = api_key)\n",
    "    res = youtube.playlistItems().list(part=\"snippet\", playlistId='UUzQTrA_c1BgRNLewVyt2UFw', maxResults=\"50\").execute()\n",
    "\n",
    "    nextPageToken = res.get('nextPageToken')\n",
    "    \n",
    "    # Creating a while structure to gather the video from the rest of the pages. \n",
    "    #It will stop when there hare no more 'nextPageToken' available\n",
    "    while ('nextPageToken' in res):\n",
    "        nextPage = youtube.playlistItems().list(\n",
    "        part=\"snippet\",\n",
    "        playlistId=playlistId,\n",
    "        maxResults=\"50\",\n",
    "        pageToken=nextPageToken\n",
    "        ).execute()\n",
    "        \n",
    "        res['items'] = res['items'] + nextPage['items']\n",
    "\n",
    "        if 'nextPageToken' not in nextPage:\n",
    "            res.pop('nextPageToken', None)\n",
    "        else:\n",
    "            nextPageToken = nextPage['nextPageToken']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6646c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the PlayListId to gather the basic data from the videos\n",
    "info_videos = gather_youtube_videos('UUzQTrA_c1BgRNLewVyt2UFw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13356af",
   "metadata": {},
   "source": [
    "Let's extract the basic information for each video from the previous dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48164fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables to store the data from the dictionary\n",
    "video_id = []\n",
    "\n",
    "# Gather all the video IDs from the dictionary\n",
    "for item in info_videos[\"items\"]:\n",
    "    video_id.append(item['snippet']['resourceId']['videoId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa2859",
   "metadata": {},
   "source": [
    "In the following step, I will gather different metrics (views, likes, shares...) with a daily granularity per video and country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74feef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the list of countries\n",
    "country = ['ES','MX','AR','BO','UY','PE','CL','CO','US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30c158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=852632511285-09e8025vv77tqnorhjgp8lof8j6j67cm.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.readonly&state=MlV19O4JiFpb7RpzmO8E0qKBNAtyf9&prompt=consent&access_type=offline\n",
      "Enter the authorization code: 4/1ARtbsJqHE7TGR-UYe4NprMrjSeNR43uKs3lS6LSxY9K49YHlIlqvrONjI-I\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary to store the data\n",
    "dic = {}\n",
    "\n",
    "# Creating a function to gather the data\n",
    "def main():\n",
    "    \n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "    api_service_name = \"youtubeAnalytics\"\n",
    "    api_version = \"v2\"\n",
    "    client_secrets_file = \"file_name.json\"\n",
    "\n",
    "    # Get credentials and create API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "    credentials = flow.run_console()\n",
    "    youtube_analytics = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    # Get the metrics for each video\n",
    "    for i in range(len(video_id)):\n",
    "        for j in range(len(country)):\n",
    "            request1 = youtube_analytics.reports().query(\n",
    "            dimensions=\"day\",\n",
    "            endDate=\"2022-10-08\",\n",
    "            ids=\"channel==MINE\",\n",
    "            maxResults=150,\n",
    "            metrics=\"views,likes,comments,dislikes,shares,subscribersLost,subscribersGained,estimatedMinutesWatched,averageViewDuration,averageViewPercentage,annotationImpressions,annotationClicks,annotationClickThroughRate,cardImpressions,cardClicks,cardClickRate\",\n",
    "            filters = f'video=={video_id[i]};country=={country[j]}',\n",
    "            startDate=\"2021-03-08\"\n",
    "    )\n",
    "            response2 = request1.execute()\n",
    "        \n",
    "            dic[video_id[i],country[j]] = response2['rows']\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af63c13",
   "metadata": {},
   "source": [
    "Now, all the data gathered has been stored in a dictionary. Let's transform that dictionary into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844b5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the variables extracted from each video\n",
    "video_metrics = ['date', 'views','likes','comments','dislikes','shares','subscribersLost','subscribersGained',\n",
    "           'estimatedMinutesWatched','averageViewDuration','averageViewPercentage','annotationImpressions',\n",
    "           'annotationClicks','annotationClickThroughRate','cardImpressions','cardClicks','cardClickRate']\n",
    "\n",
    "# Use the .items() method to store the key and the values separately in the DataFrame\n",
    "df_metrics = pd.DataFrame(dic.items())\n",
    "df_metrics\n",
    "\n",
    "# Renaming the columns\n",
    "df_metrics.rename(columns={0: 'video_id', 1: 'metrics'}, inplace=True, errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4523e7",
   "metadata": {},
   "source": [
    "I will use the function .explode() in the next step to transform each element of a list-like to a row, replicating the index values. That's why I will a reset_index() function at the end of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a49300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .explode() function to desaggregate the lists on the metrics columns\n",
    "df_stacked = df_metrics.explode('metrics').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0437b02",
   "metadata": {},
   "source": [
    "Now, let's do some final transformations before exporting this DataFrame to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab42b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_1984\\2115065348.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['video_id'] = df_stacked['video_id'].str.replace('(', '')\n",
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_1984\\2115065348.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['video_id'] = df_stacked['video_id'].str.replace(')', '')\n",
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_1984\\2115065348.py:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['metrics'] = df_stacked['metrics'].str.replace('[', '')\n",
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_1984\\2115065348.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['metrics'] = df_stacked['metrics'].str.replace(']', '')\n"
     ]
    }
   ],
   "source": [
    "# Changing the metrics variable as string to do some replacements\n",
    "df_stacked['video_id'] = df_stacked['video_id'].astype(str)\n",
    "\n",
    "# Replace the brackers by nothing\n",
    "df_stacked['video_id'] = df_stacked['video_id'].str.replace('(', '')\n",
    "df_stacked['video_id'] = df_stacked['video_id'].str.replace(')', '')\n",
    "\n",
    "# Changing the metrics variable as string to do some replacements\n",
    "df_stacked['metrics'] = df_stacked['metrics'].astype(str)\n",
    "\n",
    "# Replace the brackers by nothing\n",
    "df_stacked['metrics'] = df_stacked['metrics'].str.replace('[', '')\n",
    "df_stacked['metrics'] = df_stacked['metrics'].str.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a8451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'video_id' column into several columns\n",
    "df_stacked[['video_id2','country']] = df_stacked['video_id'].str.split(',', expand = True)\n",
    "\n",
    "# Changing 'country' column data type\n",
    "df_stacked['country'] = df_stacked['country'].astype(str)\n",
    "\n",
    "# Dropping and renaming the column video_id\n",
    "df_stacked.drop('video_id', axis = 1, inplace = True)\n",
    "df_stacked.rename(columns = {'video_id2':'video_id'}, inplace = True)\n",
    "df_stacked = df_stacked[df_stacked['metrics'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6024ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Metrics' columns into several columns\n",
    "df_stacked[video_metrics] = df_stacked['metrics'].str.split(',', expand = True)\n",
    "\n",
    "#Change Data type to remove the quotations marks correctly on the 'Date' variable\n",
    "df_stacked['date'] = df_stacked['date'].astype(str)\n",
    "\n",
    "# Replacing quotation marks by nothing\n",
    "df_stacked['video_id'] = df_stacked['video_id'].str.replace(\"'\", '')\n",
    "df_stacked['country'] = df_stacked['country'].str.replace(\"'\", '')\n",
    "df_stacked['date'] = df_stacked['date'].str.replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812e5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the old 'Metrics' variable\n",
    "df_stacked.drop('metrics', axis = 1, inplace = True)\n",
    "\n",
    "# Creating the CSV file\n",
    "df_stacked.to_csv('video_performance_country.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2e1ec",
   "metadata": {},
   "source": [
    "In the following steps, I will connect to Google BigQuery with two different goals:\n",
    "\n",
    "1. Create a table to store the data recently extracted\n",
    "2. Pull the output file got in the previous step to that Google BigQuery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba0eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to connect with Google BigQuery\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Adding the credentials from the JSON file to connect with the platform\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "'misuperproyecto-06012a0c4865.json')\n",
    "project_id = 'misuperproyecto'\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac869d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table misuperproyecto.youtube_channel_data.fact_video_country\n"
     ]
    }
   ],
   "source": [
    "# Set table_id to the ID of the table to create\n",
    "table_id = 'misuperproyecto.youtube_channel_data.fact_video_country'\n",
    "\n",
    "# Add the schema of the table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"video_id\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"country\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"date\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"views\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"likes\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"comments\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"dislikes\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"shares\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"subscribersLost\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"subscribersGained\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"estimatedMinutesWatched\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"averageViewDuration\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"averageViewPercentage\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationImpressions\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationClicks\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationClickThroughRate\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardImpressions\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardClicks\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardClickRate\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Make an API request to create the table\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  \n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354208b6",
   "metadata": {},
   "source": [
    "Finally, let's push the .csv() file created to Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f642fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2889 rows and 19 columns to misuperproyecto.youtube_channel_data.fact_video_country\n"
     ]
    }
   ],
   "source": [
    "# Specify the name of the file\n",
    "file_path = 'video_performance_country.csv'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True,\n",
    ")\n",
    "\n",
    "# Open and read the file\n",
    "with open(file_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "job.result() \n",
    "\n",
    "# Make an API request to upload the table\n",
    "table = client.get_table(table_id)  \n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ace5384e8512bec205467adaaa03d1a59280dc6777c0f016ce91eb37065280f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
